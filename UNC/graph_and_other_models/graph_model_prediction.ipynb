{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFLayer(nn.Module):\n",
    "    def __init__(self,features_out,features_in = None, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if features_in is None:\n",
    "            features_in = features_out\n",
    "        self.linear = nn.Linear(features_in,features_out,bias=bias)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class NeuralLayer(nn.Module):\n",
    "    def __init__(self, features_in,features_out,num_layers=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.linear1 = nn.Linear(features_in,features_out,bias=True)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        self.ff_layers = nn.ModuleList(\n",
    "            [FFLayer(features_out) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.ff_layers[i](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, features_in,features_out,num_layers=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.linear1 = nn.Linear(features_in,features_out,bias=True)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        self.ff_layers = nn.ModuleList(\n",
    "            [FFLayer(features_out) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = torch.matmul(adj,x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.ff_layers[i](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GraphNetwork(nn.Module):\n",
    "    def __init__(self, features_in, features_out,\n",
    "                 features_nn, num_layers,\n",
    "                 num_nn_layers, num_blocks):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        self.graphlayer1 = GraphLayer(features_in, features_out,\n",
    "                                      num_layers)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(features_out)\n",
    "        \n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [GraphLayer(features_out,features_out,num_layers)\\\n",
    "             for _ in range(num_blocks-1)])\n",
    "        \n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(features_out) for _ in range(num_blocks -1)])\n",
    "        \n",
    "        self.ff1_u = FFLayer(features_out,bias = True)\n",
    "        self.ff2_u = FFLayer(features_nn, features_out, bias = True)\n",
    "        self.nn1_u = NeuralLayer(features_nn+2,features_nn+2,num_layers = num_nn_layers)\n",
    "        \n",
    "        self.linear_u = nn.Linear(features_nn+2, 1)\n",
    "                \n",
    "    def forward(self,x,adj,lab0,lab1):\n",
    "        x= self.graphlayer1(x,adj)\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        for i in range(self.num_blocks -1):\n",
    "            res = x\n",
    "            x = self.graph_layers[i](x,adj)\n",
    "            x = torch.add(res,x)\n",
    "            x = self.norm_layers[i](x)\n",
    "        \n",
    "        y = lab0\n",
    "        z = lab1\n",
    "        u = self.ff1_u(x)\n",
    "        u = self.ff2_u(u)\n",
    "        u,_ = torch.max(u,dim = 1)\n",
    "        \n",
    "        u = torch.cat([u,torch.add(y,z),torch.abs(torch.add(y,-z))],dim = -1)\n",
    "        u = self.nn1_u(u)\n",
    "        u = nn.Sigmoid()(self.linear_u(u))\n",
    "            \n",
    "        return u\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum([param.nelement() for param in self.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, df = None, path = None, use_cuda=False, pred = False):\n",
    "        \n",
    "        if path is not None:\n",
    "            self.df = pd.read_pickle(path)\n",
    "        \n",
    "        else:\n",
    "            self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        adj = torch.tensor(self.df.iloc[idx]['adj_matrix'])\n",
    "        feat = torch.tensor(self.df.iloc[idx]['node_feature_matrix'])\n",
    "        labl = torch.tensor(self.df.iloc[idx]['labels'])\n",
    "        samp = {'adj':adj, 'feat':feat, 'labl':labl}\n",
    "        \n",
    "        return samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "def max_node(arr):\n",
    "    arr = np.sum(arr,axis = 1)\n",
    "    try:\n",
    "        m = np.where(arr==0)[0].min()\n",
    "    except ValueError:\n",
    "        m = arr.shape[0]\n",
    "    \n",
    "    return m\n",
    "\n",
    "def pad_out(arr,size):\n",
    "    diff = size-arr.shape[0]\n",
    "    return np.pad(arr,(0,diff),'constant',constant_values = 0)\n",
    "    \n",
    "def adj_norm(arr):\n",
    "    size = arr.shape[0]\n",
    "    m = max_node(arr)\n",
    "    arr = arr[:m,:m]\n",
    "    d = np.diag(np.sum(arr,axis=1))\n",
    "    d_norm = fractional_matrix_power(d, -0.5)\n",
    "    prod = np.matmul(d_norm,np.matmul(arr,d_norm))\n",
    "    return pad_out(prod,size)\n",
    "\n",
    "def direct_sum(arr1,arr2):\n",
    "    dsum = np.zeros(np.add(arr1.shape,arr2.shape))\n",
    "    dsum[:arr1.shape[0],:arr1.shape[1]]=arr1\n",
    "    dsum[arr1.shape[0]:,arr1.shape[1]:]=arr2\n",
    "    return dsum\n",
    "\n",
    "def label_shuffle(df):\n",
    "    data = df_combined[['adj_matrix','node_feature_matrix']]\n",
    "    data['labels']= df['labels'].sample(frac = 1).reset_index(drop = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_path'\n",
    "\n",
    "df = pd.read_pickle(data_path)\n",
    "df['adj_matrix'] = df.apply(lambda x: direct_sum(x['adj_matrix_x'],x['adj_matrix_y']),axis = 1)\n",
    "df['node_feature_matrix'] = df.apply(lambda x: np.concatenate((x['node_feature_matrix_x'], x['node_feature_matrix_y'])), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(df, pred = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device =  torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "data_type = torch.float32\n",
    "\n",
    "model = GraphNetwork(34,128,32,num_layers = 2,num_nn_layers = 10, num_blocks = 5)\n",
    "model.to(device, dtype= data_type)\n",
    "\n",
    "path = '../models/model_all_data_with_ac50_stand_v7'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=0, sampler = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model, test_dataloader, use_cuda = False):\n",
    "    if use_cuda is None:\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        \n",
    "    device =  torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    data_type = torch.float32\n",
    "    \n",
    "#     model.to(device, dtype= data_type)\n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "#     true = []\n",
    "    for samp in test_dataloader:\n",
    "        samp['feat']=samp['feat'].to(device,dtype=data_type)\n",
    "        samp['adj']=samp['adj'].to(device,dtype=data_type)\n",
    "        samp['labl']=samp['labl'].to(device,dtype=data_type)\n",
    "        \n",
    "        preds.append(torch.cat(list([model(samp['feat'],samp['adj'],samp['labl'][:,[0]],samp['labl'][:,[1]])]),dim = 1))\n",
    "#         true.append(samp['labl'][:,[3]])\n",
    "        \n",
    "    return torch.cat(preds).detach()#,torch.cat(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = pred(model, test_dataloader, use_cuda = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
